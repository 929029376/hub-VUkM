步骤一：文本预处理与编码 (Encoding)
Tokenizer： 使用 BERT 预训练模型的 Tokenizer 将输入文本（用户Query或知识库问题）切分为 Token 序列，添加CLS和SEP标记。
模型推断： 将 Token 序列输入 BERT 模型（如 bert-base-chinese）。BERT 会输出每一层、每一个 Token 的向量。
Pooling（池化）策略： 为了得到代表整句语义的定长向量，我们需要对 BERT 的输出进行池化。
方案 A (CLS)： 直接取最后一层CLS位置的向量。
方案 B (Mean Pooling - 推荐)： 对最后一层所有 Token 的向量取平均值。这通常比 CLS 向量更能表征句子的整体语义。

步骤二：向量存储与索引
在系统初始化或 FAQ 更新时，对所有存量的“标准问”和“相似问”进行上述编码，得到向量。将这些向量存储在向量检索引擎（如 Milvus）中，
建立索引（如 IVF_FLAT）。

步骤三：相似度计算 
当用户发起提问时，实时计算其向量。使用余弦相似度公式计算提问向量与库中向量的距离

步骤四：排序与阈值截断
根据相似度分数从高到低排序。
设定置信度阈值（例如 0.85）。
若最高分 > 0.85：直接返回对应的标准答案。
若 0.7 < 最高分 < 0.85：返回“您是否想问以下问题？”，列出前3个候选项（猜你想问）。
若最高分 < 0.7：转人工客服或提示无法理解。
